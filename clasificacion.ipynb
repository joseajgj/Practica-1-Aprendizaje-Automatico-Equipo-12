{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "wind_ava = pd.read_csv('wind_ava.csv.gz', compression=\"gzip\")\n",
    "\n",
    "# Elimina todas las variables meteorológicas que no correspondan a la localización de Sotavento (la localización 13)\n",
    "wind_ava = wind_ava.filter(regex='^(datetime|energy|.*\\.13)$')\n",
    "\n",
    "# Elimina las variables 'lai_hv.13' y 'v10n.13'\n",
    "if 'lai_hv.13' in wind_ava.columns and 'v10n.13' in wind_ava.columns:\n",
    "    wind_ava = wind_ava.drop(['lai_hv.13', 'v10n.13'], axis=1)\n",
    "\n",
    "# Vuelve a definir las variables numéricas\n",
    "numerical_vars = wind_ava.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Quitamos \"energy\" de las variables numéricas, ya que querremos usarla como variable objetivo\n",
    "numerical_vars = numerical_vars.drop('energy')\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(wind_ava[numerical_vars], wind_ava['energy'], test_size=1/3, random_state=100472166, shuffle=False)\n",
    "\n",
    "# Crear el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transformar las características del conjunto de prueba usando el mismo escalador\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dividir los datos de entrenamiento en un conjunto de entrenamiento más pequeño y un conjunto de validación\n",
    "X_train_inner, X_val, y_train_inner, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=100472166)\n",
    "\n",
    "# Definir el modelo SVM con los hiperparámetros seleccionados\n",
    "best_model = SVR(C=1000.0, shrinking=True, degree=2, gamma='auto')\n",
    "\n",
    "# Ajustar el modelo SVM en el conjunto de entrenamiento más pequeño\n",
    "best_model.fit(X_train_inner, y_train_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar utilizaremos nuestro mejor modelo hasta el momento para comprobar si las predicciones para valores altos son peores que para valores bajos (o viceversa). Esto lo haremos comparando su RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predecir los valores de la variable objetivo para el conjunto de validación\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calcular el error cuadrático medio (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Dividir las predicciones y los valores reales en grupos de altos y bajos\n",
    "# Aquí, asumimos que \"altos\" son aquellos valores mayores a la mediana y \"bajos\" son los menores\n",
    "median_energy = np.median(y_val)\n",
    "y_val_high = y_val[y_val > median_energy]\n",
    "y_val_low = y_val[y_val <= median_energy]\n",
    "y_pred_high = y_pred[y_val > median_energy]\n",
    "y_pred_low = y_pred[y_val <= median_energy]\n",
    "\n",
    "# Calcular el RMSE para los valores altos y bajos\n",
    "rmse_high = np.sqrt(mean_squared_error(y_val_high, y_pred_high))\n",
    "rmse_low = np.sqrt(mean_squared_error(y_val_low, y_pred_low))\n",
    "\n",
    "print(f\"RMSE para valores altos: {rmse_high}\")\n",
    "print(f\"RMSE para valores bajos: {rmse_low}\")\n",
    "\n",
    "# Comprobar si las predicciones para valores altos son peores que para valores bajos\n",
    "if rmse_high > rmse_low:\n",
    "    print(\"Las predicciones para valores altos son peores que para valores bajos.\")\n",
    "else:\n",
    "    print(\"Las predicciones para valores altos no son necesariamente peores que para valores bajos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión a clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Eliminar la columna \"datetime\" del conjunto de datos\n",
    "wind_ava.drop([\"datetime\"], axis=1, inplace=True)\n",
    "\n",
    "# Determinar el tercer cuantil de la variable objetivo\n",
    "energy_quantile = wind_ava['energy'].quantile(0.25)\n",
    "\n",
    "# Clasificar los valores de energía como \"baja\" o \"alta\"\n",
    "wind_ava['energy_class'] = ['baja' if e < energy_quantile else 'alta' for e in wind_ava['energy']]\n",
    "\n",
    "# Convertir las etiquetas de clase a números\n",
    "label_encoder = LabelEncoder()\n",
    "wind_ava['energy_class_encoded'] = label_encoder.fit_transform(wind_ava['energy_class'])\n",
    "\n",
    "# Almacenamos en una variable todas las columnas no relacionadas con la variable objetivo\n",
    "excluir_columnas = ['energy', 'energy_class', 'energy_class_encoded']\n",
    "numerical_vars = [col for col in wind_ava.columns if col not in excluir_columnas]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    wind_ava[numerical_vars], \n",
    "    wind_ava['energy_class_encoded'], \n",
    "    test_size=1/3, \n",
    "    random_state=100472166\n",
    ")\n",
    "\n",
    "# Ajustar el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "scaler = StandardScaler()\n",
    "X_train_class_scaled = scaler.fit_transform(X_train_class)\n",
    "X_test_class_scaled = scaler.transform(X_test_class)\n",
    "\n",
    "# Definir y entrenar un modelo de clasificación\n",
    "classifier = RandomForestClassifier(random_state=100472166)\n",
    "classifier.fit(X_train_class_scaled, y_train_class)\n",
    "\n",
    "# Predecir las clases para el conjunto de prueba\n",
    "y_pred_class = classifier.predict(X_test_class_scaled)\n",
    "\n",
    "# Calcular la precisión del modelo de clasificación\n",
    "accuracy = classifier.score(X_test_class_scaled, y_test_class)\n",
    "print(f\"Precisión del modelo de clasificación: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wind_ava[numerical_vars], \n",
    "    wind_ava['energy_class_encoded'], \n",
    "    test_size=1/3, \n",
    "    random_state=100472166\n",
    ")\n",
    "\n",
    "# Definir el modelo\n",
    "model = RandomForestClassifier(random_state=100472166)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda en cuadrícula para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las clases para el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar las métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos distintos\n",
    "\n",
    "Podemos probar a realizar el ajuste de hiperparámetros también para otros modelos diferentes al Random Forest Classifier, como SVC o los árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Hiperparámetros para SVC\n",
    "svc_params = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'gamma': np.logspace(-3, 3, 7),\n",
    "    'shrinking': [True, False]\n",
    "    # 'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Hiperparámetros para DecisionTreeClassifier\n",
    "dt_params = {\n",
    "    'max_depth': np.arange(1, 15),\n",
    "    'min_samples_split': np.arange(2, 15),\n",
    "    'min_samples_leaf': np.arange(1, 15),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Hiperparámetros para KNeighborsClassifier\n",
    "knn_params = {\n",
    "    'n_neighbors': np.arange(1, 15),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'leaf_size': np.arange(1, 15),\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Ajustar el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVC\n",
    "svc = SVC(random_state=42)\n",
    "svc_search = GridSearchCV(svc, svc_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "svc_search.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc_search.predict(X_test_scaled)\n",
    "print(\"Mejores hiperparámetros para SVC:\", svc_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(\"------------------------\")\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt_search = GridSearchCV(dt, dt_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "dt_search.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt_search.predict(X_test_scaled)\n",
    "print(\"Mejores hiperparámetros para DecisionTreeClassifier:\", dt_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(\"------------------------\")\n",
    "\n",
    "# KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_search = GridSearchCV(knn, knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "knn_search.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_search.predict(X_test_scaled)\n",
    "print(\"Mejores hiperparámetros para KNeighborsClassifier:\", knn_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
