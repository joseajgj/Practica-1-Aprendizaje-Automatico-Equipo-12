{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos y filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "wind_ava = pd.read_csv('wind_ava.csv.gz', compression=\"gzip\")\n",
    "\n",
    "# Elimina todas las variables meteorológicas que no correspondan a la localización de Sotavento (la localización 13)\n",
    "wind_ava = wind_ava.filter(regex='^(datetime|energy|.*\\.13)$')\n",
    "\n",
    "# Elimina las variables 'lai_hv.13' y 'v10n.13'\n",
    "if 'lai_hv.13' in wind_ava.columns and 'v10n.13' in wind_ava.columns:\n",
    "    wind_ava = wind_ava.drop(['lai_hv.13', 'v10n.13'], axis=1)\n",
    "\n",
    "# Vuelve a definir las variables numéricas\n",
    "numerical_vars = wind_ava.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Quitamos \"energy\" de las variables numéricas, ya que querremos usarla como variable objetivo\n",
    "numerical_vars = numerical_vars.drop('energy')\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(wind_ava[numerical_vars], wind_ava['energy'], test_size=1/3, random_state=100472166, shuffle=False)\n",
    "\n",
    "# Crea el escalador\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajusta el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transforma las características del conjunto de prueba usando el mismo escalador\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Divide los datos de entrenamiento en un conjunto de entrenamiento más pequeño y un conjunto de validación\n",
    "X_train_inner, X_val, y_train_inner, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=100472166)\n",
    "\n",
    "# Define el modelo SVM con los hiperparámetros seleccionados\n",
    "best_model = SVR(C=1000.0, coef0= 0, gamma= 'scale', kernel= 'rbf', shrinking= True)\n",
    "\n",
    "# Ajusta el modelo SVM en el conjunto de entrenamiento más pequeño\n",
    "best_model.fit(X_train_inner, y_train_inner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar utilizaremos nuestro mejor modelo hasta el momento para comprobar si las predicciones para valores altos son peores que para valores bajos (o viceversa). Esto lo haremos comparando su RSME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predice los valores de la variable objetivo para el conjunto de validación\n",
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "# Calcula el error cuadrático medio (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Divide las predicciones y los valores reales en grupos de altos y bajos\n",
    "# Aquí, asumimos que \"altos\" son aquellos valores mayores a la mediana y \"bajos\" son los menores\n",
    "median_energy = np.median(y_val)\n",
    "y_val_high = y_val[y_val > median_energy]\n",
    "y_val_low = y_val[y_val <= median_energy]\n",
    "y_pred_high = y_pred[y_val > median_energy]\n",
    "y_pred_low = y_pred[y_val <= median_energy]\n",
    "\n",
    "# Calcula el RMSE para los valores altos y bajos\n",
    "rmse_high = np.sqrt(mean_squared_error(y_val_high, y_pred_high))\n",
    "rmse_low = np.sqrt(mean_squared_error(y_val_low, y_pred_low))\n",
    "\n",
    "print(f\"RMSE para valores altos: {rmse_high}\")\n",
    "print(f\"RMSE para valores bajos: {rmse_low}\")\n",
    "\n",
    "# Comprueba si las predicciones para valores altos son peores que para valores bajos\n",
    "if rmse_high > rmse_low:\n",
    "    print(\"Las predicciones para valores altos son peores que para valores bajos.\")\n",
    "else:\n",
    "    print(\"Las predicciones para valores altos no son necesariamente peores que para valores bajos.\")\n",
    "\n",
    "# Crea una figura con un tamaño específico (ancho, alto)\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "# Crear una lista con los nombres de los grupos\n",
    "groups = ['RMSE General', 'RMSE Valores Altos', 'RMSE Valores Bajos']\n",
    "\n",
    "# Crear una lista con los RMSE de cada grupo\n",
    "rmse_values = [rmse, rmse_high, rmse_low]\n",
    "\n",
    "# Crear un diagrama de barras para los RMSE\n",
    "ax.bar(groups, rmse_values, color=['blue', 'green', 'red'])\n",
    "ax.set_title('Comparación de RMSE')\n",
    "ax.set_ylabel('RMSE')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión a clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Eliminar la columna \"datetime\" del conjunto de datos\n",
    "wind_ava.drop([\"datetime\"], axis=1, inplace=True)\n",
    "\n",
    "# Determinar el tercer cuantil de la variable objetivo\n",
    "energy_quantile = wind_ava['energy'].quantile(0.25)\n",
    "\n",
    "# Clasificar los valores de energía como \"baja\" o \"alta\"\n",
    "wind_ava['energy_class'] = ['baja' if e < energy_quantile else 'alta' for e in wind_ava['energy']]\n",
    "\n",
    "# Convertir las etiquetas de clase a números\n",
    "label_encoder = LabelEncoder()\n",
    "wind_ava['energy_class_encoded'] = label_encoder.fit_transform(wind_ava['energy_class'])\n",
    "\n",
    "# Almacenamos en una variable todas las columnas no relacionadas con la variable objetivo\n",
    "excluir_columnas = ['energy', 'energy_class', 'energy_class_encoded']\n",
    "numerical_vars = [col for col in wind_ava.columns if col not in excluir_columnas]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    wind_ava[numerical_vars], \n",
    "    wind_ava['energy_class_encoded'], \n",
    "    test_size=1/3, \n",
    "    random_state=100472166\n",
    ")\n",
    "\n",
    "# Ajustar el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "scaler = StandardScaler()\n",
    "X_train_class_scaled = scaler.fit_transform(X_train_class)\n",
    "X_test_class_scaled = scaler.transform(X_test_class)\n",
    "\n",
    "# Definir y entrenar un modelo de clasificación\n",
    "classifier = RandomForestClassifier(random_state=100472166)\n",
    "classifier.fit(X_train_class_scaled, y_train_class)\n",
    "\n",
    "# Predecir las clases para el conjunto de prueba\n",
    "y_pred_class = classifier.predict(X_test_class_scaled)\n",
    "\n",
    "# Calcular la precisión del modelo de clasificación\n",
    "accuracy = classifier.score(X_test_class_scaled, y_test_class)\n",
    "print(f\"Precisión del modelo de clasificación: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros y Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    wind_ava[numerical_vars], \n",
    "    wind_ava['energy_class_encoded'], \n",
    "    test_size=1/3, \n",
    "    random_state=100472166\n",
    ")\n",
    "\n",
    "# Definir el modelo\n",
    "model = RandomForestClassifier(random_state=100472166)\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda en cuadrícula para encontrar los mejores hiperparámetros\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print(\"Mejores hiperparámetros: \", grid_search.best_params_)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las clases para el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcular y mostrar las métricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Crear una figura con un tamaño específico (ancho, alto)\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Crear un mapa de calor para la matriz de confusión\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "\n",
    "# Configurar los títulos y las etiquetas\n",
    "ax.set_title('Matriz de Confusión')\n",
    "ax.set_xlabel('Predicción')\n",
    "ax.set_ylabel('Real')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos distintos\n",
    "\n",
    "Podemos probar a realizar el ajuste de hiperparámetros también para otros modelos diferentes al Random Forest Classifier, como SVC o los árboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Hiperparámetros para SVC\n",
    "svc_params = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'gamma': np.logspace(-3, 3, 7),\n",
    "    'shrinking': [True, False]\n",
    "    # 'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Hiperparámetros para DecisionTreeClassifier\n",
    "dt_params = {\n",
    "    'max_depth': np.arange(1, 15),\n",
    "    'min_samples_split': np.arange(2, 15),\n",
    "    'min_samples_leaf': np.arange(1, 15),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Hiperparámetros para KNeighborsClassifier\n",
    "knn_params = {\n",
    "    'n_neighbors': np.arange(1, 15),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'leaf_size': np.arange(1, 15),\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Ajustar el escalador a las características del conjunto de entrenamiento y transformarlas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVC\n",
    "svc = SVC(random_state=100472166)\n",
    "svc_search = GridSearchCV(svc, svc_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "svc_search.fit(X_train_scaled, y_train)\n",
    "y_pred_svc = svc_search.predict(X_test_scaled)\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "print(\"Mejores hiperparámetros para SVC:\", svc_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "print(cm_svc)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(random_state=100472166)\n",
    "dt_search = GridSearchCV(dt, dt_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "dt_search.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt_search.predict(X_test_scaled)\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "print(\"Mejores hiperparámetros para DecisionTreeClassifier:\", dt_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(cm_dt)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_search = GridSearchCV(knn, knn_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "knn_search.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_search.predict(X_test_scaled)\n",
    "print(\"Mejores hiperparámetros para KNeighborsClassifier:\", knn_search.best_params_)\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print(cm_knn)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Crear una figura con un tamaño específico (ancho, alto)\n",
    "fig, axs = plt.subplots(3, figsize=(12, 18))\n",
    "\n",
    "# Crear mapas de calor para las matrices de confusión\n",
    "sns.heatmap(cm_svc, annot=True, fmt='d', cmap='Blues', ax=axs[0])\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', ax=axs[1])\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues', ax=axs[2])\n",
    "\n",
    "# Configurar los títulos y las etiquetas\n",
    "axs[0].set_title('Matriz de Confusión para SVC')\n",
    "axs[0].set_xlabel('Predicción')\n",
    "axs[0].set_ylabel('Real')\n",
    "\n",
    "axs[1].set_title('Matriz de Confusión para DecisionTreeClassifier')\n",
    "axs[1].set_xlabel('Predicción')\n",
    "axs[1].set_ylabel('Real')\n",
    "\n",
    "axs[2].set_title('Matriz de Confusión para KNeighborsClassifier')\n",
    "axs[2].set_xlabel('Predicción')\n",
    "axs[2].set_ylabel('Real')\n",
    "\n",
    "# Mostrar la figura\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los usos que se le ha dado a ChatGPT ha sido conseguir bases de las que partir. Obviamente tuvimos que reescribir prácticamente todo ese código, ya que ChatGPT generaba un código muy incompleto y que nunca exploraba opciones más allá de lo que se le pedía. Sin embargo, esto nos permitió ahorrar tiempo al no tener que escribir a mano todas las tareas sencillas y repetitivas, tiempo que pudimos usar en seguir mejorando nuestros modelos finales.\n",
    "\n",
    "También nos fue de gran utilidad cuando lo usamos para explorar opciones más allá de las vistas en clase. Este fue el caso, por ejemplo, del ajuste de hiperparámetros. Para conseguir los mejores resultados posibles, le pedimos a ChatGPT que nos dijese todos los posibles parámetros que admitía cada modelo y cuáles eran los valores que solían tomar. Después de eso, fuimos probando con varios de ellos hasta que estuvimos contentos con los que teníamos, ya fuera en términos de MSE o de tiempo de ejecución. Por lo tanto, esto no solo nos permitió ajustar el modelo con más precisión, si no que además ahorramos tiempo al ya tener una idea de los rangos de valores y no tener que probarlos todos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos de los prompts en cuestión fueron:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En un modelo de regresión qué HPO admiten los modelos SVM, KNN, Árboles de decisión, Regresión lineal y Lasso? Dame todos los posibles parametros que se puedan ajustar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cabe destacar que al realizar esta pregunta el ChatGPT confundía hiperparámetros y valores que podrían tomar entre los diferentes tipos de modelos, pero al final obtuvimos esta respuesta:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM (Support Vector Machines)\n",
    "\n",
    "- **C**: Parámetro de regularización.\n",
    "- **kernel**: Tipo de kernel a utilizar.\n",
    "- **degree**: Grado del polinomio en el caso de 'poly'.\n",
    "- **gamma**: Coeficiente para el kernel 'rbf', 'poly' y 'sigmoid'.\n",
    "- **coef0**: Coeficiente independiente en 'poly' y 'sigmoid'.\n",
    "- **shrinking**: Especifica si se debe utilizar el algoritmo de reducción.\n",
    "- **probability**: Especifica si se debe calcular la probabilidad ajustada.\n",
    "- **tol**: Tolerancia para la parada del algoritmo.\n",
    "- **cache_size**: Tamaño de la caché en MB.\n",
    "- **class_weight**: Pesos de las clases.\n",
    "- **verbose**: Nivel de verbosidad.\n",
    "- **max_iter**: Número máximo de iteraciones.\n",
    "- **decision_function_shape**: Forma de la función de decisión.\n",
    "- **break_ties**: Método para romper empates en la decisión.\n",
    "- **random_state**: Semilla para la generación de números aleatorios.\n",
    "\n",
    "KNN (K-Nearest Neighbors)\n",
    "\n",
    "- **n_neighbors**: Número de vecinos a considerar.\n",
    "- **weights**: Función de ponderación de los vecinos.\n",
    "- **algorithm**: Algoritmo utilizado para calcular los vecinos más cercanos.\n",
    "- **leaf_size**: Tamaño de los árboles construidos por el algoritmo 'ball_tree' o 'kd_tree'.\n",
    "- **p**: Parámetro de la métrica de distancia.\n",
    "- **metric**: Métrica de distancia utilizada.\n",
    "- **metric_params**: Parámetros adicionales para la métrica de distancia.\n",
    "- **n_jobs**: Número de trabajos paralelos.\n",
    "\n",
    "Árboles de Decisión (Decision Trees)\n",
    "\n",
    "- **criterion**: Función a utilizar para medir la calidad de una división.\n",
    "- **splitter**: Algoritmo a utilizar para encontrar la división de los nodos.\n",
    "- **max_depth**: Profundidad máxima del árbol.\n",
    "- **min_samples_split**: Número mínimo de muestras requeridas para dividir un nodo interno.\n",
    "- **min_samples_leaf**: Número mínimo de muestras requeridas para estar en un nodo hoja.\n",
    "- **min_weight_fraction_leaf**: Fración mínima de peso total en un nodo hoja.\n",
    "- **max_features**: Número de características a considerar al buscar la mejor división.\n",
    "- **random_state**: Semilla para la generación de números aleatorios.\n",
    "- **max_leaf_nodes**: Número máximo de nodos hoja.\n",
    "- **min_impurity_decrease**: Impureza mínima requerida para una división.\n",
    "- **class_weight**: Pesos de las clases.\n",
    "- **presort**: Especifica si se debe preordenar los datos.\n",
    "\n",
    "Regresión Lineal (Linear Regression)\n",
    "\n",
    "- **fit_intercept**: Especifica si se debe ajustar un término de intercepción.\n",
    "- **normalize**: Especifica si se debe normalizar los datos antes del ajuste.\n",
    "- **copy_X**: Especifica si se debe copiar los datos de entrada.\n",
    "- **n_jobs**: Número de trabajos paralelos.\n",
    "- **positive**: Especifica si se debe asegurar que los coeficientes sean positivos.\n",
    "\n",
    "Lasso (L1 Regularization)\n",
    "\n",
    "- **alpha**: Coeficiente de regularización.\n",
    "- **fit_intercept**: Especifica si se debe ajustar un término de intercepción.\n",
    "- **normalize**: Especifica si se debe normalizar los datos antes del ajuste.\n",
    "- **precompute**: Especifica si se debe precalcular la matriz de Gram.\n",
    "- **max_iter**: Número máximo de iteraciones.\n",
    "- **tol**: Tolerancia para la parada del algoritmo.\n",
    "- **warm_start**: Especifica si se debe reutilizar la solución anterior como punto de partida.\n",
    "- **positive**: Especifica si se debe asegurar que los coeficientes sean positivos.\n",
    "- **random_state**: Semilla para la generación de números aleatorios.\n",
    "- **selection**: Método de selección de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo puedo agrupar los gráfios que se generan en el siguiente código de forma individual en uno solo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentado para que no se ejecute\n",
    "'''import matplotlib.pyplot as plt\n",
    "\n",
    "for col in numerical_vars:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    wind_ava[col].hist(bins=30, alpha=0.5, label=col)\n",
    "    plt.title(f'Distribución de {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show() '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentado para que no se ejecute\n",
    "'''# Número de variables numéricas\n",
    "num_vars = len(numerical_vars)\n",
    "\n",
    "# Calcular el número de filas y columnas para los subplots\n",
    "num_cols = 4  # número máximo de columnas\n",
    "num_rows = np.ceil(num_vars / num_cols).astype(int)\n",
    "\n",
    "# Crear una figura y un conjunto de subtramas\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(6*num_cols, 4*num_rows))\n",
    "\n",
    "# Aplanar el array de subplots para facilitar su recorrido\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Para cada variable numérica, crear un histograma en su respectivo subplot\n",
    "for i, col in enumerate(numerical_vars):\n",
    "    axs[i].hist(wind_ava[col], bins=30, alpha=0.5, label=col)\n",
    "    axs[i].set_title(f'Distribución de {col}')\n",
    "    axs[i].set_xlabel(col)\n",
    "    axs[i].set_ylabel('Frecuencia')\n",
    "    axs[i].legend(loc='upper right')\n",
    "\n",
    "# Eliminar los subplots vacíos si el número de variables no es múltiplo de 4\n",
    "if num_vars % num_cols != 0:\n",
    "    for j in range(i+1, num_rows*num_cols):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "# Ajustar el espacio entre los subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar la figura con todos los histogramas\n",
    "plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
